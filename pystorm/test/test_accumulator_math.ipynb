{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nengo_brainstorm.solvers import CVXSolver\n",
    "from nengo_extras.plot_spikes import plot_spikes\n",
    "\n",
    "import pystorm\n",
    "from pystorm.hal import HAL\n",
    "from pystorm.hal.net_builder import NetBuilder\n",
    "from pystorm.hal.run_control import RunControl\n",
    "from pystorm.hal.data_utils import lpf, bin_to_spk_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0,
     19
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_snr_gamma(lamtau_out, k):\n",
    "    \"\"\"SNR of the synaptically filtered gamma process\n",
    "\n",
    "    In terms of the output lambda * tau\n",
    "    \"\"\"\n",
    "    if isinstance(lamtau_out, float):\n",
    "        lamtau_out = np.array([lamtau_out])\n",
    "    lamtau_in = lamtau_out * k\n",
    "    snr = np.zeros_like(lamtau_out)\n",
    "    idx = lamtau_out > 0\n",
    "    x = lamtau_in\n",
    "    a = np.sqrt(2*x[idx])\n",
    "    b_num = (1+x[idx])**k+x[idx]**k\n",
    "    b_den = (1+x[idx])**k-x[idx]**k\n",
    "    b = b_num/b_den\n",
    "    c = 2*x[idx]/k\n",
    "    snr[idx] = a / np.sqrt(k*(b-c))\n",
    "    return snr\n",
    "\n",
    "def get_snr_periodic(lamtau):\n",
    "    snr = np.zeros_like(lamtau)\n",
    "    idx = lamtau > 0\n",
    "    snr[idx] = 1./np.sqrt(\n",
    "        1./(2.*lamtau[idx])*(1+np.exp(-1/(lamtau[idx])))/(1-np.exp(-1/(lamtau[idx])))-1)\n",
    "    return snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set parameters for network\n",
    "X = 16\n",
    "Y = 8\n",
    "NNEURON = X*Y\n",
    "DIM = 1\n",
    "FMAX = 1000\n",
    "DOWNSTREAM_NS = 10000\n",
    "\n",
    "hal = pystorm.hal.HAL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_builder = NetBuilder(hal)\n",
    "\n",
    "def build_taps(net_builder):\n",
    "    bad_syn, _ = net_builder.determine_bad_syns()\n",
    "    SX = X // 2\n",
    "    SY = Y // 2\n",
    "    bad_syn = bad_syn[:SY, :SX]\n",
    "    tap_matrix_syn = net_builder.create_default_yx_taps(SY, SX, DIM, bad_syn)\n",
    "    tap_matrix = net_builder.syn_taps_to_nrn_taps(tap_matrix_syn)\n",
    "    np.savetxt(\"tap_matrix.txt\", tap_matrix)\n",
    "    return tap_matrix\n",
    "\n",
    "tap_matrix = build_taps(net_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_net(net_builder, tap_matrix):\n",
    "    gain_divs = np.loadtxt(\"gain_divisors.txt\", dtype=int)\n",
    "    biases = np.loadtxt(\"biases.txt\", dtype=int)\n",
    "\n",
    "    d_matrix = np.zeros((Y*X+1, Y*X))\n",
    "    d_matrix[:-1] = np.eye(Y*X)\n",
    "    net = net_builder.create_single_pool_net(\n",
    "        Y, X, tap_matrix, biases=biases, gain_divs=gain_divs, decoders=d_matrix)\n",
    "    return net\n",
    "\n",
    "def assign_decoders(net, decoders, hal):\n",
    "    net.decoder_conn.weights[-1, :] = decoders\n",
    "    net.decoder_conn.reassign_weights(net.decoder_conn.weights)\n",
    "    hal.remap_weights()\n",
    "\n",
    "net = build_net(net_builder, tap_matrix)\n",
    "run_control = RunControl(hal, net)\n",
    "hal.map(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Decoders\n",
    "\n",
    "- Check that tuning curves look reasonable\n",
    "- Check that accumulator spikes match raw spikes via identity decode matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class TrainInfo:\n",
    "    def __init__(self, train_input_rates, spike_rates):\n",
    "        self.train_input_rates = train_input_rates\n",
    "        self.spike_rates = spike_rates\n",
    "\n",
    "def collect_train_data(net, hal, fmax, run_control):\n",
    "    bin_size = 0.5 # seconds\n",
    "    bin_size_ns = int(bin_size*1E9)\n",
    "    hal.set_time_resolution(DOWNSTREAM_NS, bin_size_ns)\n",
    "\n",
    "    total_train_points = 11\n",
    "\n",
    "    train_input_rates = np.zeros((total_train_points+2, 1))\n",
    "    train_input_rates[1:total_train_points+1,0] = fmax * np.linspace(-1, 1, total_train_points)\n",
    "    train_input_rates[0, 0] = train_input_rates[1, 0] # add padding to account for slop in trials\n",
    "    train_input_rates[-1, 0] = train_input_rates[-2, 0]\n",
    "    train_time_ns = np.arange(total_train_points+2)*bin_size_ns\n",
    "    input_data = {net.input:(train_time_ns, train_input_rates)}\n",
    "\n",
    "    output_data, _ = run_control.run_input_sweep(\n",
    "        input_data, get_raw_spikes=False, get_outputs=True)\n",
    "    outputs, output_times = output_data\n",
    "    outputs = outputs[net.output][1:, :-1]\n",
    "    spike_rates = outputs/bin_size\n",
    "    train_input_rates = train_input_rates[1:-1]\n",
    "    \n",
    "    tinfo = TrainInfo(train_input_rates, spike_rates)\n",
    "    return tinfo\n",
    "tinfo = collect_train_data(net, hal, FMAX, run_control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tuning(inputs, spike_rates, array_width, array_height):\n",
    "    half_width = array_width//2\n",
    "    for idx in range(array_height):\n",
    "        start_l = idx*array_width\n",
    "        start_r = start_l + half_width\n",
    "        plt.plot(inputs, spike_rates[:, start_l:start_l+half_width], 'r')\n",
    "        plt.plot(inputs, spike_rates[:, start_r:start_r+half_width], 'b')\n",
    "plot_tuning(tinfo.train_input_rates, tinfo.spike_rates, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_decoders(rates, target_function):\n",
    "    solver = CVXSolver(reg=0.1, reg_l1=0.1)\n",
    "    decoders, info = solver(rates, target_function)\n",
    "    decoders = decoders.clip(-1, 1)\n",
    "    rmse = info['rmses']\n",
    "    print(rmse)\n",
    "    return decoders\n",
    "\n",
    "tinfo.target_function = tinfo.train_input_rates + FMAX\n",
    "decoders = fit_decoders(tinfo.spike_rates, tinfo.target_function)\n",
    "tinfo.decoders = decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_fit(train_input_rates, target_function, spike_rates, decoders):\n",
    "    train_decode = np.dot(spike_rates, decoders)\n",
    "    plt.figure()\n",
    "    plt.plot(train_input_rates, target_function, label=\"target function\")\n",
    "    plt.plot(train_input_rates, train_decode, label=\"decoded function\")\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    z_idx = np.searchsorted(train_input_rates[:, 0], 0) # input 0\n",
    "    rates_0 = spike_rates[z_idx] # spike rates at input 0\n",
    "    plt.figure()\n",
    "    plt.hist(decoders[rates_0>0], density=True, bins=40)\n",
    "\n",
    "plot_training_fit(tinfo.train_input_rates, tinfo.target_function, tinfo.spike_rates, tinfo.decoders.flatten())    \n",
    "print(np.sum(np.abs(tinfo.decoders)<1E-2))\n",
    "assign_decoders(net, tinfo.decoders.flatten(), hal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Decoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  deliver an input of 0\n",
    "def run_test(hal, run_control, bin_size=0.0001):\n",
    "    \"\"\"Run a single input test trial\"\"\"\n",
    "    bin_size_ns = int(bin_size*1E9)\n",
    "    hal.set_time_resolution(DOWNSTREAM_NS, bin_size_ns)\n",
    "    \n",
    "    test_time = 1\n",
    "    test_time_ns = int(test_time*1E9)\n",
    "\n",
    "    input_rates = np.zeros((2, 1))\n",
    "    input_times = np.arange(2)*test_time_ns\n",
    "\n",
    "    input_vals = {net.input:(input_times, input_rates)}\n",
    "    output_data, _ = run_control.run_input_sweep(\n",
    "        input_vals, get_raw_spikes=False, get_outputs=True)\n",
    "\n",
    "    outputs, bin_times_ns = output_data\n",
    "    outputs = outputs[net.output]\n",
    "    decode = outputs[:, -1]\n",
    "    spikes = outputs[:, :-1]\n",
    "    \n",
    "    bin_times = bin_times_ns * 1E-9\n",
    "    bin_times -= bin_times[0]\n",
    "    return decode, spikes, bin_times\n",
    "decode, spikes, bin_times = run_test(hal, run_control)\n",
    "if np.sum(spikes[0]) > 2*spikes.shape[1]: # zero-out spikes that accumulated between traffic activation and exp\n",
    "    spikes[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process output data\n",
    "def check_decode(decode):\n",
    "    bins_gt0 = np.sum(decode>0)\n",
    "    total_outputs = np.sum(decode)\n",
    "    bins_1 = np.sum(decode==1)\n",
    "    bins_2 = np.sum(decode==2)\n",
    "    bins_gt2 = np.sum(decode>2)\n",
    "    bin_vals_gt2 = np.unique(decode[decode>2])\n",
    "    print(\"Collected {:d} non-zero output bins. Sum(outputs) = {:d}\".format(bins_gt0, total_outputs))\n",
    "    print(\"Bin stats:1-spike bins: {:d}, 2-spike bins: {:d}, >2-spike bins: {:d} (bin values {})\".format(\n",
    "        bins_1, bins_2, bins_gt2, bin_vals_gt2))\n",
    "check_decode(decode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a raster of spikes and outputs\n",
    "to_raster = np.zeros((decode.shape[0], 1+spikes.shape[1]))\n",
    "to_raster[:, :-1] = spikes\n",
    "to_raster[:, -1] = decode\n",
    "to_raster[to_raster>1] = 1\n",
    "plt.subplots(figsize=(16, 12))\n",
    "plot_spikes(bin_times, to_raster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = bin_times[1] - bin_times[0]\n",
    "tau = 0.01\n",
    "\n",
    "valid_decode = decode.copy()\n",
    "valid_decode[decode>10] = 0\n",
    "filtered_decode = lpf(valid_decode, tau, dt)\n",
    "\n",
    "filtered_spikes = lpf(spikes, tau, dt)\n",
    "decoded_spikes = spikes*decoders[:, 0]\n",
    "filtered_decoded_spikes = lpf(decoded_spikes, tau, dt)\n",
    "decode = np.sum(filtered_decoded_spikes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = np.mean(filtered_decode[bin_times>5*tau])\n",
    "var = np.var(filtered_decode[bin_times>5*tau])\n",
    "print(mean/np.sqrt(var))\n",
    "\n",
    "plt.subplots(figsize=(10,4))\n",
    "plt.plot(bin_times, filtered_decode, label=\"filtered accumulator output\")\n",
    "plt.plot(bin_times, decode, label=\"filtered, decode-weighted, raw spikes via identity matrix\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.xlabel(\"time\")\n",
    "\n",
    "plt.subplots(figsize=(10,4))\n",
    "for idx in range(X*Y):\n",
    "    plt.plot(bin_times, filtered_spikes[:, idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixed, All Positive Decode Weights\n",
    "\n",
    "- compare to all weights positive and equal\n",
    "- check for poissonness of superposed spike trains\n",
    "- sweep decoder magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DData:\n",
    "    def __init__(self, decode, spikes, bin_times, dweights):\n",
    "        self.decode = decode\n",
    "        self.spikes = spikes\n",
    "        self.bin_times = bin_times\n",
    "        self.dweights = dweights\n",
    "    \n",
    "def dw_sweep_collect_data(dweights, net, run_control, hal, labels=None):\n",
    "    \"\"\"Collect data from experiments that sweep across dweights\"\"\"\n",
    "    dw_data = {}\n",
    "    if labels is not None:\n",
    "        assert len(dweights) == len(labels)\n",
    "        diter = zip(dweights, labels)\n",
    "    else:\n",
    "        diter = zip(dweights, dweights)\n",
    "    for dw, label in diter:\n",
    "        assign_decoders(net, dw, hal)\n",
    "        decode, spikes, bin_times = run_test(hal, run_control, bin_size=0.0001)\n",
    "        \n",
    "        check_decode(decode)\n",
    "        sticky_decode_idx = decode > 100000\n",
    "        if sticky_decode_idx.any():\n",
    "            print(\"zeroing out {} sticky bitted decode bins\".format(np.sum(sticky_decode_idx)))\n",
    "        decode[sticky_decode_idx] = 0\n",
    "    \n",
    "        sticky_spk_idx = spikes > 100000\n",
    "        if sticky_spk_idx.any():\n",
    "            print(\"zeroing out {} sticky bitted spike bins\".format(np.sum(sticky_spk_idx)))\n",
    "            spikes[sticky_spk_idx] = 0\n",
    "\n",
    "        dw_data[label] = DData(decode=decode, spikes=spikes, bin_times=bin_times, dweights=dw)\n",
    "    return dw_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hal.set_time_resolution(downstream_ns=10000, upstream_ns=100000)\n",
    "dweights = [1, 1/2, 1/4, 1/8, 1/16, 1/32, 1/64]\n",
    "# dweights = [1, 1/2, 1/4]\n",
    "ddata = dw_sweep_collect_data(dweights, net, run_control, hal)\n",
    "for dw in ddata: # clean up sticky bitted spikes if present\n",
    "    if (ddata[dw].spikes > 100000).any():\n",
    "        ddata[dw].spikes[ddata[dw].spikes > 100000] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAU = 0.01\n",
    "def dw_sweep_analyze_data(ddata):\n",
    "    for dw in ddata:\n",
    "        decode = ddata[dw].decode\n",
    "        spikes = ddata[dw].spikes\n",
    "        bin_times = ddata[dw].bin_times\n",
    "        dt = bin_times[1] - bin_times[0]\n",
    "        filtered_decode = lpf(decode, TAU, dt)\n",
    "        idx = bin_times>5*TAU\n",
    "        fin = np.sum(spikes[idx]) / (bin_times[idx][-1] - bin_times[idx][0])\n",
    "        fout = np.sum(decode[idx]) / (bin_times[idx][-1] - bin_times[idx][0])\n",
    "        mean = np.mean(filtered_decode[idx])\n",
    "        var = np.var(filtered_decode[idx])\n",
    "        snr = mean/np.sqrt(var)\n",
    "        print(\"dw {} fin {:.0f} fout {:.0f} mean {:.0f} var {:.0f} snr {:.2f}\".format(\n",
    "            dw, fin, fout, mean, var, snr))\n",
    "        dspk_times = bin_to_spk_times(decode, bin_times)\n",
    "        isi = np.diff(dspk_times)\n",
    "        isi_cv = np.sqrt(np.var(isi)) / np.mean(isi)\n",
    "        \n",
    "        ddata[dw].filtered_decode = filtered_decode\n",
    "        ddata[dw].fin, ddata[dw].fout = (fin, fout)\n",
    "        ddata[dw].mean, ddata[dw].snr = (mean, snr)\n",
    "        ddata[dw].dspk_times = dspk_times\n",
    "        ddata[dw].isi = isi\n",
    "        ddata[dw].isi_cv = isi_cv\n",
    "    return ddata\n",
    "ddata = dw_sweep_analyze_data(ddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     21
    ]
   },
   "outputs": [],
   "source": [
    "def dw_sweep_plot_data(ddata):\n",
    "    fig_stats, axs_stats = plt.subplots(ncols=2, nrows=2, figsize=(16, 12))\n",
    "    ax_snr, ax_cv = axs_stats[0]\n",
    "    ax_snr_check = axs_stats[1, 0]\n",
    "    fig_exp, axs_exp = plt.subplots(ncols=2, figsize=(14,4))\n",
    "    ax_filt, ax_f = axs_exp\n",
    "    fig_hist, axs_hist = plt.subplots(nrows=len(ddata), figsize=(14, 2*len(ddata)), sharex=True)\n",
    "    \n",
    "    fout = np.zeros(len(ddata))\n",
    "    fin = np.zeros(len(ddata))\n",
    "    snr = np.zeros(len(ddata))\n",
    "    snr_th = np.zeros(len(ddata))\n",
    "    isi_cv = np.zeros(len(ddata))\n",
    "    for idx, dw in enumerate(ddata):\n",
    "        color = ax_filt.plot(ddata[dw].bin_times, ddata[dw].filtered_decode)[0].get_color()\n",
    "        ax_filt.axhline(ddata[dw].mean, color=\"k\", alpha=0.5)\n",
    "        fin[idx] = ddata[dw].fin\n",
    "        fout[idx] = ddata[dw].fout\n",
    "        snr[idx] = ddata[dw].snr\n",
    "        snr_th[idx] = get_snr_gamma(ddata[dw].fout*TAU, 1/dw)\n",
    "        isi_cv[idx] = ddata[dw].isi_cv\n",
    "        axs_hist[idx].hist(ddata[dw].isi, bins=50, cumulative=False, density=True, histtype=\"step\", color=color)\n",
    "\n",
    "    ax_filt.set_title('do means look reasonable?')\n",
    "    fends = np.array([fout.min(), fout.max()])\n",
    "    snr_poi = np.sqrt(2*fends*TAU)\n",
    "    snr_per_high_lt_appx = np.sqrt(12)*fends*TAU\n",
    "    snr_per = get_snr_periodic(fout*TAU)\n",
    "\n",
    "    ax_snr.loglog(fout, snr, 'o', label=\"observed snr\")\n",
    "    ax_snr.loglog(fout, snr_th, '-o', label=\"theoretical gamma snr\")\n",
    "    ax_snr.loglog(fends, snr_poi, label=\"theoretical poisson snr\")\n",
    "    per_color = ax_snr.loglog(fout, snr_per, label=\"theoretical periodic snr\")[0].get_color()\n",
    "    ax_snr.loglog(fends, snr_per_high_lt_appx, color=per_color, alpha=0.2)\n",
    "    ax_snr.legend(loc=\"best\")\n",
    "    ax_snr.set_xlabel(\"f_out\")\n",
    "    ax_snr.set_ylabel(\"SNR\")\n",
    "    ax_snr.grid(which=\"both\")\n",
    "    \n",
    "    rel_snr = np.abs(snr-snr_th)/snr_th\n",
    "    ax_snr_check.semilogx(fout, rel_snr, '-o')\n",
    "    ax_snr_check.grid()\n",
    "    ax_snr_check.set_xlabel(\"f_out\")\n",
    "    ax_snr_check.set_ylabel(\"|SNR - SNR_theory| / SNR_theory\")\n",
    "    \n",
    "    ax_cv.semilogx(fout, isi_cv, '-o')\n",
    "    ax_cv.set_ylim([0, 1])\n",
    "    ax_cv.set_xlabel(\"f_out\")\n",
    "    ax_cv.set_ylabel(\"CV\")\n",
    "    ax_cv.grid(which=\"both\")\n",
    "\n",
    "    ax_f.plot(fin, '-o', label=\"f_in\")\n",
    "    ax_f.plot(fout, '-o', label=\"f_out\")\n",
    "    ax_f.legend(loc=\"best\")\n",
    "    ax_f.set_title(\"is f_in constant across trials?\")    \n",
    "\n",
    "dw_sweep_plot_data(ddata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trained Decoders\n",
    "\n",
    "- What should f_in be?\n",
    "- What should the decode weight be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build decoder sets to try\n",
    "fmaxes = np.linspace(5000, 100, 5)\n",
    "decoder_sets = []\n",
    "for idx, fmax in enumerate(fmaxes):\n",
    "    target_function = tinfo.train_input_rates + fmax\n",
    "    decoder_sets.append(fit_decoders(tinfo.spike_rates, target_function).flatten())\n",
    "#     print(decoder_sets[-1])\n",
    "    print(np.sum(decoder_sets[-1]==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddata = dw_sweep_collect_data(decoder_sets, net, run_control, hal, labels=fmaxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TAU = 0.001\n",
    "def fmax_sweep_analyze_data(ddata):\n",
    "    for fmax in ddata:\n",
    "        decode = ddata[fmax].decode\n",
    "        spikes = ddata[fmax].spikes\n",
    "        bin_times = ddata[fmax].bin_times\n",
    "        dt = bin_times[1] - bin_times[0]\n",
    "        filtered_decode = lpf(decode, TAU, dt)\n",
    "        idx = bin_times>5*TAU\n",
    "        fin = np.sum(spikes[idx]) / (bin_times[idx][-1] - bin_times[idx][0])\n",
    "        fout = np.sum(decode[idx]) / (bin_times[idx][-1] - bin_times[idx][0])\n",
    "        mean = np.mean(filtered_decode[idx])\n",
    "        var = np.var(filtered_decode[idx])\n",
    "        snr = mean/np.sqrt(var)\n",
    "        print(\"fmax {} fin {:.0f} fout {:.0f} mean {:.0f} var {:.0f} snr {:.2f}\".format(\n",
    "            fmax, fin, fout, mean, var, snr))\n",
    "        dspk_times = bin_to_spk_times(decode, bin_times)\n",
    "        isi = np.diff(dspk_times)\n",
    "        isi_cv = np.sqrt(np.var(isi)) / np.mean(isi)\n",
    "        \n",
    "        ddata[fmax].filtered_decode = filtered_decode\n",
    "        ddata[fmax].fin, ddata[fmax].fout = (fin, fout)\n",
    "        ddata[fmax].mean, ddata[fmax].snr = (mean, snr)\n",
    "        ddata[fmax].dspk_times = dspk_times\n",
    "        ddata[fmax].isi = isi\n",
    "        ddata[fmax].isi_cv = isi_cv\n",
    "    return ddata\n",
    "\n",
    "ddata = fmax_sweep_analyze_data(ddata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fmax_sweep_plot_data(ddata):\n",
    "    fig_stats, axs_stats = plt.subplots(ncols=2, nrows=2, figsize=(16, 12))\n",
    "    ax_snr, ax_cv = axs_stats[0]\n",
    "    ax_snr_check = axs_stats[1, 0]\n",
    "    fig_exp, axs_exp = plt.subplots(ncols=2, figsize=(14,4))\n",
    "    ax_filt, ax_f = axs_exp\n",
    "    fig_hist, axs_hist = plt.subplots(nrows=len(ddata), figsize=(14, 2*len(ddata)), sharex=True)\n",
    "    fig_dw, axs_dw = plt.subplots(nrows=len(ddata), figsize=(14, 2*len(ddata)), sharex=True)\n",
    "    \n",
    "    fout = np.zeros(len(ddata))\n",
    "    fin = np.zeros(len(ddata))\n",
    "    snr = np.zeros(len(ddata))\n",
    "    snr_th = np.zeros(len(ddata))\n",
    "    isi_cv = np.zeros(len(ddata))\n",
    "    fmax_tgt = np.zeros(len(ddata))\n",
    "    for idx, fmax in enumerate(ddata):\n",
    "        color = ax_filt.plot(ddata[fmax].bin_times, ddata[fmax].filtered_decode)[0].get_color()\n",
    "        ax_filt.axhline(ddata[fmax].mean, color=\"k\", alpha=0.5)\n",
    "        fin[idx] = ddata[fmax].fin\n",
    "        fout[idx] = ddata[fmax].fout\n",
    "        snr[idx] = ddata[fmax].snr\n",
    "        fmax_tgt[idx] = fmax\n",
    "        effective_dw = ddata[fmax].fout / ddata[fmax].fin\n",
    "        snr_th[idx] = get_snr_gamma(ddata[fmax].fout*TAU, 1/effective_dw)\n",
    "        isi_cv[idx] = ddata[fmax].isi_cv\n",
    "        axs_hist[idx].hist(\n",
    "            ddata[fmax].isi, bins=50, cumulative=False, density=True, histtype=\"step\", color=color,\n",
    "            label=\"target fmax={}\".format(fmax))\n",
    "        axs_hist[idx].legend(loc=\"upper right\")\n",
    "        axs_dw[idx].hist(\n",
    "            ddata[fmax].dweights, bins=50, cumulative=False, density=True, histtype=\"step\", color=color,\n",
    "            label=\"target fmax={}\".format(fmax))\n",
    "        axs_dw[idx].legend(loc=\"upper right\")\n",
    "    ax_filt.set_title('do means look reasonable?')\n",
    "    ax_filt.set_xlabel(\"time\")\n",
    "    ax_filt.set_ylabel(\"filtered decode\")\n",
    "    axs_hist[0].set_title(\"ISI Distributions\")\n",
    "    axs_hist[-1].set_xlabel(\"ISIs\")\n",
    "    axs_dw[0].set_title(\"Decode Weight Distributions\")\n",
    "    axs_dw[-1].set_xlabel(\"Decode Weights\")\n",
    "    axs_dw[-1].set_xlim([-1.1, 1.1])\n",
    "    fends = np.array([fout.min(), fout.max()])\n",
    "    snr_poi = np.sqrt(2*fends*TAU)\n",
    "    snr_per_high_lt_appx = np.sqrt(12)*fends*TAU\n",
    "    snr_per = get_snr_periodic(fout*TAU)\n",
    "\n",
    "    ax_snr.loglog(fout, snr, 'o', label=\"observed snr\")\n",
    "    ax_snr.loglog(fout, snr_th, '-o', label=\"theoretical gamma snr\")\n",
    "    ax_snr.loglog(fends, snr_poi, label=\"theoretical poisson snr\")\n",
    "    per_color = ax_snr.loglog(fout, snr_per, label=\"theoretical periodic snr\")[0].get_color()\n",
    "    ax_snr.loglog(fends, snr_per_high_lt_appx, color=per_color, alpha=0.2)\n",
    "    ax_snr.legend(loc=\"best\")\n",
    "    ax_snr.set_xlabel(\"f_out\")\n",
    "    ax_snr.set_ylabel(\"SNR\")\n",
    "    ax_snr.grid(which=\"both\")\n",
    "    \n",
    "    rel_snr = np.abs(snr-snr_th)/snr_th\n",
    "    ax_snr_check.semilogx(fout, rel_snr, '-o')\n",
    "    ax_snr_check.grid()\n",
    "    ax_snr_check.set_xlabel(\"f_out\")\n",
    "    ax_snr_check.set_ylabel(\"|SNR - SNR_theory| / SNR_theory\")\n",
    "    \n",
    "    ax_cv.semilogx(fout, isi_cv, '-o')\n",
    "    ax_cv.set_ylim([0, 1])\n",
    "    ax_cv.set_xlabel(\"f_out\")\n",
    "    ax_cv.set_ylabel(\"CV\")\n",
    "    ax_cv.grid(which=\"both\")\n",
    "\n",
    "    ax_f.plot(fmax_tgt, fin, '-o', label=\"f_in\")\n",
    "    ax_f.plot(fmax_tgt, fout, '-o', label=\"f_out\")\n",
    "    ax_f.legend(loc=\"best\")\n",
    "    ax_f.set_title(\"is f_in constant across trials?\")    \n",
    "    ax_f.set_xlabel(\"target fmax\")\n",
    "\n",
    "fmax_sweep_plot_data(ddata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratchspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def th_snr_sweep():\n",
    "    fins = np.linspace(1000, 10000, 5)\n",
    "    dweights = np.linspace(0.015, 1, 100)\n",
    "    fouts = np.zeros((len(dweights), len(fins)))\n",
    "    routs = np.zeros_like(fouts)\n",
    "    plt.figure()\n",
    "    for fidx, fin in enumerate(fins):\n",
    "        for didx, dw in enumerate(dweights):\n",
    "            fouts[didx, fidx] = fin*dw\n",
    "            routs[didx, fidx] = get_snr_gamma(fouts[didx, fidx]*TAU, 1/dw)\n",
    "        plt.loglog(fouts[:, fidx], routs[:, fidx])\n",
    "    fends = np.array([fouts.min(), fouts.max()])\n",
    "    r_uni = np.sqrt(12)*fends*TAU\n",
    "    r_poi = np.sqrt(2*fends*TAU)\n",
    "    plt.loglog(fends, r_uni)\n",
    "    plt.loglog(fends, r_poi)\n",
    "    plt.ylim([plt.ylim()[0], 1.1*routs.max()])\n",
    "th_snr_sweep()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
