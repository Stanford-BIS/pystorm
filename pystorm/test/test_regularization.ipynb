{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook description\n",
    "\n",
    "Decoders $d$ are found via optimization to minimize the mean-squared error between then desired function and the decode-weighted tuning curve as well as the L2 and L1 norms of the decode weights themselves.\n",
    "\n",
    "$$\\arg\\min_d \\|f-Ad\\|_2^2 + \\lambda_{L2}\\|d\\|_2 + \\lambda_{L1}\\|d\\|_1$$\n",
    "\n",
    " - Generate realistic tuning curves from hardware\n",
    " - Specify an exemplary target function to decode (e.g. $f(x)=f_{max}(x+1)$)\n",
    " - Sweep the space of $\\lambda_{L2}$ and $\\lambda_{L1}$, finding decoder sets for each $\\lambda_{L1}$ $\\lambda_{L2}$ pair\n",
    " - Examine the decode validation error, input frequency (i.e., energy), nonzero decode weights, and SNR at 0 input for each $\\lambda_{L1}$ $\\lambda_{L2}$ pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nengo_brainstorm.solvers import CVXSolver\n",
    "\n",
    "import pystorm\n",
    "from pystorm.hal import HAL\n",
    "from pystorm.hal.net_builder import NetBuilder\n",
    "from pystorm.hal.run_control import RunControl\n",
    "from pystorm.hal.data_utils import lpf, bin_to_spk_times, bins_to_rates\n",
    "\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for network\n",
    "X = 16\n",
    "Y = 8\n",
    "NNEURON = X*Y\n",
    "DIM = 1\n",
    "FMAX = 1000\n",
    "DOWNSTREAM_NS = 10000\n",
    "UPSTREAM_NS   = 100000\n",
    "hal = pystorm.hal.HAL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_builder = NetBuilder(hal)\n",
    "\n",
    "def build_taps(net_builder):\n",
    "    bad_syn, _ = net_builder.determine_bad_syns()\n",
    "    SX = X // 2\n",
    "    SY = Y // 2\n",
    "    bad_syn = bad_syn[:SY, :SX]\n",
    "    tap_matrix_syn = net_builder.create_default_yx_taps(SY, SX, DIM, bad_syn)\n",
    "    tap_matrix = net_builder.syn_taps_to_nrn_taps(tap_matrix_syn)\n",
    "    np.savetxt(\"tap_matrix.txt\", tap_matrix)\n",
    "    return tap_matrix\n",
    "\n",
    "tap_matrix = build_taps(net_builder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_net(net_builder, tap_matrix, d_matrix):\n",
    "    gain_divs = np.loadtxt(\"gain_divisors.txt\", dtype=int)\n",
    "    biases = np.loadtxt(\"biases.txt\", dtype=int)\n",
    "    d_matrix = np.eye(Y*X)\n",
    "    net = net_builder.create_single_pool_net(\n",
    "        Y, X, tap_matrix, biases=biases, gain_divs=gain_divs, decoders=d_matrix)\n",
    "    return net\n",
    "\n",
    "tuning_net = build_net(net_builder, tap_matrix, np.eye(Y*X))\n",
    "run_control = RunControl(hal, tuning_net)\n",
    "hal.map(tuning_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_tuning_data(net, hal, fmax, run_control):\n",
    "    bin_size = 1.0 # seconds\n",
    "    bin_size_ns = int(bin_size*1E9)\n",
    "    hal.set_time_resolution(DOWNSTREAM_NS, UPSTREAM_NS)\n",
    "\n",
    "    total_points = 21\n",
    "\n",
    "    input_rates = np.zeros((total_points+1, 1))\n",
    "    input_rates[:total_points, 0] = fmax * np.linspace(-1, 1, total_points)\n",
    "    input_rates[-1, 0] = input_rates[-2, 0]\n",
    "    time_ns = np.arange(total_points+1)*bin_size_ns\n",
    "    input_data = {net.input:(time_ns, input_rates)}\n",
    "    output_data, _ = run_control.run_input_sweep(\n",
    "        input_data, get_raw_spikes=False, get_outputs=True)\n",
    "    outputs, output_times = output_data\n",
    "    outputs = outputs[net.output][:, :-1] # last dimension reserved for decode\n",
    "    spike_rates = bins_to_rates(outputs, output_times, time_ns, init_discard_frac=0.5)\n",
    "    input_rates = input_rates[:-1]\n",
    "    return input_rates, spike_rates\n",
    "\n",
    "input_rates, spike_rates = collect_tuning_data(tuning_net, hal, FMAX, run_control)\n",
    "# split into training and validation data sets\n",
    "train_input_rates = input_rates[0::2]\n",
    "valid_input_rates = input_rates[1::2]\n",
    "train_spike_rates = spike_rates[0::2]\n",
    "valid_spike_rates = spike_rates[1::2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tuning(inputs, spike_rates, array_width, array_height):\n",
    "    half_width = array_width//2\n",
    "    plt.figure()\n",
    "    for idx in range(array_height):\n",
    "        start_l = idx*array_width\n",
    "        start_r = start_l + half_width\n",
    "        plt.plot(inputs, spike_rates[:, start_l:start_l+half_width], 'r')\n",
    "        plt.plot(inputs, spike_rates[:, start_r:start_r+half_width], 'b')\n",
    "plot_tuning(train_input_rates, train_spike_rates, X, Y)\n",
    "plt.title(\"training\")\n",
    "plot_tuning(valid_input_rates, valid_spike_rates, X, Y)\n",
    "plt.title(\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_decoders(rates, target_function, l1, l2):\n",
    "    solver = CVXSolver(reg=l2, reg_l1=l1)\n",
    "    decoders, info = solver(rates, target_function)\n",
    "    decoders = decoders.clip(-1, 1)\n",
    "    return decoders, info\n",
    "\n",
    "train_target_function = train_input_rates + FMAX\n",
    "valid_target_function = valid_input_rates + FMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up L1 L2 space to sweep\n",
    "N_L1 = 2\n",
    "N_L2 = 2\n",
    "L1_vals = np.linspace(0, 0.1, N_L1)\n",
    "L2_vals = np.linspace(0, 0.1, N_L2)\n",
    "\n",
    "L1_grid, L2_grid = np.meshgrid(L1_vals, L2_vals)\n",
    "L1L2_pts = np.zeros((N_L1*N_L2, 2))\n",
    "L1L2_pts[:, 0] = L1_grid.flatten()\n",
    "L1L2_pts[:, 1] = L2_grid.flatten()\n",
    "\n",
    "snr = np.zeros((N_L1, N_L2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute decoders\n",
    "decoders = []\n",
    "rmse_train = []\n",
    "for l1, l2 in L1L2_pts:\n",
    "    dec, info = fit_decoders(train_spike_rates, train_target_function, l1, l2)\n",
    "    decoders.append(dec)\n",
    "    rmse_train.append(info['rmses'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute stats\n",
    "nz_dw_threshold = 1/(8192*2)\n",
    "print(nz_dw_threshold)\n",
    "\n",
    "rmse_valid = []\n",
    "f_in = []\n",
    "nz_dw = []\n",
    "for dweights in decoders:\n",
    "    nz_idx = np.abs(dweights.flatten())>nz_dw_threshold\n",
    "    nz_dw.append(np.sum(nz_idx))\n",
    "    f_in.append(np.sum(np.mean(valid_spike_rates[:, nz_idx], axis=0)))\n",
    "    decode = np.dot(valid_spike_rates, dweights)\n",
    "    rmse_valid.append(np.sqrt(np.mean((valid_target_function-decode)**2)))\n",
    "print(nz_dw)\n",
    "print(f_in)\n",
    "print(rmse_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "rmse_train = np.reshape(rmse_train, (N_L2, N_L1))\n",
    "rmse_train = np.reshape(rmse_valid, (N_L2, N_L1))\n",
    "f_in = np.reshape(f_in, (N_L2, N_L1))\n",
    "nz_dw = np.reshape(nz_dw, (N_L2, N_L1))\n",
    "\n",
    "fig_rmse, axs_rmse = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "ax_rmse_train, ax_rmse_valid = axs_rmse\n",
    "ax_rmse_train.contour(L1_grid, L2_grid, rmse_train)\n",
    "ax_rmse_valid.contour(L1_grid, L2_grid, rmse_train)\n",
    "\n",
    "fig_energy, axs_energy = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "ax_fin, ax_nzdw = axs_energy\n",
    "ax_fin.contour(L1_grid, L2_grid, f_in)\n",
    "ax_nzdw.contour(L1_grid, L2_grid, nz_dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect SNR data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_training_fit(train_input_rates, target_function, spike_rates, decoders):\n",
    "#     train_decode = np.dot(spike_rates, decoders)\n",
    "#     plt.figure()\n",
    "#     plt.plot(train_input_rates, target_function, label=\"target function\")\n",
    "#     plt.plot(train_input_rates, train_decode, label=\"decoded function\")\n",
    "#     plt.legend(loc=\"best\")\n",
    "\n",
    "#     z_idx = np.searchsorted(train_input_rates[:, 0], 0) # input 0\n",
    "#     rates_0 = spike_rates[z_idx] # spike rates at input 0\n",
    "#     plt.figure()\n",
    "#     plt.hist(decoders[rates_0>0], density=True, bins=40)\n",
    "\n",
    "# plot_training_fit(tinfo.train_input_rates, tinfo.target_function, tinfo.spike_rates, tinfo.decoders.flatten())    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
