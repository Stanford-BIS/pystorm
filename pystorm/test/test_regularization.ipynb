{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook description\n",
    "\n",
    "Decoders $d$ are found via optimization to minimize the mean-squared error between then desired function and the decode-weighted tuning curve as well as the L2 and L1 norms of the decode weights themselves.\n",
    "\n",
    "$$\\arg\\min_d \\|f-Ad\\|_2^2 + \\lambda_{L2}\\|d\\|_2 + \\lambda_{L1}\\|d\\|_1$$\n",
    "\n",
    " - Generate realistic tuning curves from hardware\n",
    " - Specify an exemplary target function to decode (e.g. $f(x)=f_{max}(x+1)$)\n",
    " - Sweep the space of $\\lambda_{L2}$ and $\\lambda_{L1}$, finding decoder sets for each $\\lambda_{L1}$ $\\lambda_{L2}$ pair\n",
    " - Examine the decode validation error, input frequency (i.e., energy), nonzero decode weights, and SNR at 0 input for each $\\lambda_{L1}$ $\\lambda_{L2}$ pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from nengo_brainstorm.solvers import CVXSolver\n",
    "\n",
    "import pystorm\n",
    "from pystorm.hal import HAL\n",
    "from pystorm.hal.net_builder import NetBuilder\n",
    "from pystorm.hal.run_control import RunControl\n",
    "from pystorm.hal.data_utils import lpf, bin_to_spk_times, bins_to_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters for network\n",
    "X = 8\n",
    "Y = 8\n",
    "NNEURON = X*Y\n",
    "DIM = 1\n",
    "FMAX = 1000\n",
    "DOWNSTREAM_NS = 10000\n",
    "UPSTREAM_NS   = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hal = pystorm.hal.HAL()\n",
    "net_builder = NetBuilder(hal)\n",
    "\n",
    "def build_taps(net_builder):\n",
    "    bad_syn, _ = net_builder.determine_bad_syns()\n",
    "    SX = X // 2\n",
    "    SY = Y // 2\n",
    "    bad_syn = bad_syn[:SY, :SX]\n",
    "    tap_matrix_syn = net_builder.create_default_yx_taps(SY, SX, DIM, bad_syn)\n",
    "    tap_matrix = net_builder.syn_taps_to_nrn_taps(tap_matrix_syn)\n",
    "    np.savetxt(\"tap_matrix.txt\", tap_matrix)\n",
    "    return tap_matrix\n",
    "\n",
    "tap_matrix = build_taps(net_builder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_net(net_builder, tap_matrix, d_matrix):\n",
    "    gain_divs = np.loadtxt(\"gain_divisors.txt\", dtype=int)\n",
    "    biases = np.loadtxt(\"biases.txt\", dtype=int)\n",
    "    d_matrix = np.eye(Y*X)\n",
    "    net = net_builder.create_single_pool_net(\n",
    "        Y, X, tap_matrix, biases=biases, gain_divs=gain_divs, decoders=d_matrix)\n",
    "    return net\n",
    "\n",
    "tuning_net = build_net(net_builder, tap_matrix, np.eye(Y*X))\n",
    "run_control = RunControl(hal, tuning_net)\n",
    "hal.map(tuning_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_tuning_data(net, hal, fmax, run_control, total_points):\n",
    "    bin_size = 0.5 # seconds\n",
    "    bin_size_ns = int(bin_size*1E9)\n",
    "    hal.set_time_resolution(DOWNSTREAM_NS, UPSTREAM_NS)\n",
    "    input_rates = np.zeros((total_points+1, 1))\n",
    "    input_rates[:total_points, 0] = fmax * np.linspace(-1, 1, total_points)\n",
    "    input_rates[-1, 0] = input_rates[-2, 0]\n",
    "    time_ns = np.arange(total_points+1)*bin_size_ns\n",
    "    input_data = {net.input:(time_ns, input_rates)}\n",
    "    output_data, _ = run_control.run_input_sweep(\n",
    "        input_data, get_raw_spikes=False, get_outputs=True)\n",
    "    outputs, output_times = output_data\n",
    "    outputs = outputs[net.output][:, :-1] # last dimension reserved for decode\n",
    "    spike_rates = bins_to_rates(outputs, output_times, time_ns, init_discard_frac=0.25)\n",
    "    input_rates = input_rates[:-1]\n",
    "    return input_rates, spike_rates\n",
    "\n",
    "def collect_fake_tuning_data(nn, total_points, fmax, max_spike_rate, sigma=20):\n",
    "    input_values = np.linspace(-1, 1, total_points)\n",
    "    biases = np.zeros(nn)\n",
    "    biases[:nn//2] = np.linspace(-1, 1, nn//2)\n",
    "    biases[nn//2:] = np.linspace(-1, 1, nn//2)\n",
    "    input_rates = fmax * input_values.reshape((-1, 1))\n",
    "    spike_rates = np.zeros((total_points, nn))\n",
    "    spike_rates[:, :nn//2] = max_spike_rate * (input_values.reshape((-1, 1)) + biases[:nn//2])\n",
    "    spike_rates[:, nn//2:] = max_spike_rate * (-input_values.reshape((-1, 1)) + biases[nn//2:])\n",
    "    spike_rates += sigma*np.random.randn(*spike_rates.shape)\n",
    "    spike_rates[spike_rates < 0] = 0\n",
    "    return input_rates, spike_rates\n",
    "    \n",
    "input_rates, spike_rates = collect_tuning_data(tuning_net, hal, FMAX, run_control, total_points=241)\n",
    "# input_rates, spike_rates = collect_fake_tuning_data(X*Y, total_points=101, fmax=FMAX, max_spike_rate=500)\n",
    "\n",
    "# split into training and validation data sets\n",
    "r_valid = 4\n",
    "v_idx = np.zeros(len(input_rates), dtype=bool)\n",
    "v_idx[r_valid-r_valid//2::4] = True\n",
    "train_input_rates = input_rates[~v_idx]\n",
    "valid_input_rates = input_rates[v_idx]\n",
    "train_spike_rates = spike_rates[~v_idx]\n",
    "valid_spike_rates = spike_rates[v_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_tuning(inputs, spike_rates, array_width, array_height):\n",
    "    half_width = array_width//2\n",
    "    plt.figure()\n",
    "    for idx in range(array_height):\n",
    "        start_l = idx*array_width\n",
    "        start_r = start_l + half_width\n",
    "        plt.plot(inputs, spike_rates[:, start_l:start_l+half_width], 'r')\n",
    "        plt.plot(inputs, spike_rates[:, start_r:start_r+half_width], 'b')\n",
    "plot_tuning(train_input_rates, train_spike_rates, X, Y)\n",
    "plt.title(\"training\")\n",
    "plt.xlabel(\"input rate\")\n",
    "plt.ylabel(\"spike rate\")\n",
    "plot_tuning(valid_input_rates, valid_spike_rates, X, Y)\n",
    "plt.title(\"validation\")\n",
    "plt.xlabel(\"input rate\")\n",
    "plt.ylabel(\"spike rate\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_decoders(rates, target_function, l1, l2):\n",
    "    solver = CVXSolver(reg=l2, reg_l1=l1)\n",
    "    decoders, info = solver(rates, target_function)\n",
    "    decoders = decoders.clip(-1, 1)\n",
    "    return decoders, info\n",
    "\n",
    "# train_target = train_input_rates + FMAX\n",
    "# valid_target = valid_input_rates + FMAX\n",
    "\n",
    "def target_function(rates, freq=0.5):\n",
    "    return FMAX*(np.sin(2*np.pi*freq*rates/FMAX))\n",
    "train_target = target_function(train_input_rates)\n",
    "valid_target = target_function(valid_input_rates)\n",
    "\n",
    "def plot_target_fcn():\n",
    "    plt.plot(train_input_rates, train_target, 'o', label=\"training targets ({})\".format(len(train_input_rates)))\n",
    "    plt.plot(valid_input_rates, valid_target, 'o', label=\"validation targets ({})\".format(len(valid_input_rates)))\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.xlabel(\"Input Spike Rate\")\n",
    "plot_target_fcn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up L1 L2 space to sweep\n",
    "# N_L1 = 25\n",
    "# N_L2 = 25\n",
    "# L1_vals = np.linspace(0.1, 10.0, N_L1)\n",
    "# L2_vals = np.linspace(0.1, 10.0, N_L2)\n",
    "\n",
    "N_L1 = 11\n",
    "N_L2 = 11\n",
    "L1_vals = np.logspace(-1, 6, N_L1)\n",
    "L2_vals = np.logspace(-1, 6, N_L2)\n",
    "\n",
    "\n",
    "L1_grid, L2_grid = np.meshgrid(L1_vals, L2_vals)\n",
    "L1L2_pts = np.zeros((N_L1*N_L2, 2))\n",
    "L1L2_pts[:, 0] = L1_grid.flatten()\n",
    "L1L2_pts[:, 1] = L2_grid.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute decoders\n",
    "decoders = []\n",
    "rmse_train = []\n",
    "for l1, l2 in L1L2_pts:\n",
    "    dec, info = fit_decoders(train_spike_rates, train_target, l1, l2)\n",
    "    decoders.append(dec)\n",
    "    rmse_train.append(info['rmses'])\n",
    "\n",
    "# from multiprocessing import Pool\n",
    "# def mp_fit_decoders(l1l2):\n",
    "#     l1, l2 = l1l2\n",
    "#     return fit_decoders(train_spike_rates, train_target, l1, l2)\n",
    "# mp_pool = Pool(processes=4)\n",
    "# ret = mp_pool.map(mp_fit_decoders, L1L2_pts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute stats\n",
    "nz_dw_threshold = 1/(2*64)\n",
    "\n",
    "rmse_valid = []\n",
    "f_in = []\n",
    "nz_dw = []\n",
    "mean_abs_dw = []\n",
    "decodes = []\n",
    "for dweights in decoders:\n",
    "    nz_idx = np.abs(dweights.flatten())>nz_dw_threshold\n",
    "    nz_dw.append(np.sum(nz_idx))\n",
    "    mean_abs_dw.append(np.mean(np.abs(dweights)))\n",
    "    f_in.append(np.sum(np.mean(valid_spike_rates[:, nz_idx], axis=0)))\n",
    "    decode = np.dot(valid_spike_rates, dweights)\n",
    "    decodes.append(decode)\n",
    "    plt.plot(valid_input_rates, decode)\n",
    "    rmse_valid.append(np.sqrt(np.mean((valid_target-decode)**2)))\n",
    "plt.plot(valid_input_rates, valid_target, 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "rmse_train = np.reshape(rmse_train, (N_L2, N_L1))\n",
    "rmse_valid = np.reshape(rmse_valid, (N_L2, N_L1))\n",
    "f_in = np.reshape(f_in, (N_L2, N_L1))\n",
    "nz_dw = np.reshape(nz_dw, (N_L2, N_L1))\n",
    "mean_abs_dw = np.reshape(mean_abs_dw, (N_L2, N_L1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decodes(valid_input_rates, valid_target, decodes):\n",
    "    fig, axs = plt.subplots(nrows=N_L2, ncols=N_L1, sharex=True, sharey=True, figsize=(14, 12))\n",
    "    for d_idx, decode in enumerate(decodes):\n",
    "        r_idx = N_L2 - (d_idx // N_L1) - 1\n",
    "        c_idx = d_idx % N_L1\n",
    "        axs[r_idx][c_idx].plot(valid_input_rates, valid_target)\n",
    "        axs[r_idx][c_idx].plot(valid_input_rates, decode)\n",
    "    axs[0, 0].set_xticks([])\n",
    "    axs[0, 0].set_yticks([])\n",
    "plot_decodes(valid_input_rates, valid_target, decodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contour(X, Y, Z, ax, fig, contour_args={}, cb_args={}):\n",
    "    cset = ax.contourf(X, Y, Z, **contour_args)\n",
    "    ax.contour(X, Y, Z, cset.levels, colors=\"k\", linewidths=1)\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    cb = fig.colorbar(cset, ax=ax, **cb_args)\n",
    "    ax.set_xlabel(\"L1 Scaling\")\n",
    "    ax.set_ylabel(\"L2 Scaling\")\n",
    "    \n",
    "def plot_rmse(L1, L2, train, valid):\n",
    "    norm = cm.colors.Normalize(vmin=np.min((rmse_train, rmse_valid)), vmax=np.max((rmse_train, rmse_valid)))\n",
    "    fig, _axs = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "    ax_train, ax_valid = _axs\n",
    "    norm = cm.colors.Normalize(vmin=np.min((train, valid)), vmax=np.max((train, valid)))\n",
    "    plot_contour(L1, L2, train, ax_train, fig, dict(norm=norm))\n",
    "    plot_contour(L1, L2, valid, ax_valid, fig, dict(norm=norm))\n",
    "    ax_train.set_title(\"Training NRMSE\")\n",
    "    ax_valid.set_title(\"Validation NRMSE\")\n",
    "plot_rmse(L1_grid, L2_grid, rmse_train/FMAX, rmse_valid/FMAX)\n",
    "\n",
    "def plot_energy(L1, L2, f_in, nz_dw, mean_abs_dw):\n",
    "    fig, _axs = plt.subplots(ncols=3, figsize=(16, 4))\n",
    "    ax_fin, ax_nzdw, ax_madw = _axs\n",
    "    plot_contour(L1, L2, f_in, ax_fin, fig)\n",
    "    plot_contour(L1, L2, nz_dw, ax_nzdw, fig)\n",
    "    plot_contour(L1, L2, mean_abs_dw, ax_madw, fig)\n",
    "    ax_fin.set_title(\"f_in\")\n",
    "    ax_nzdw.set_title(\"Nonzero Decode Weights\")\n",
    "    ax_madw.set_title(\"Mean Absolute Decode Weight\")\n",
    "plot_energy(L1_grid, L2_grid, f_in, nz_dw, mean_abs_dw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect SNR data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_training_fit(train_input_rates, target_function, spike_rates, decoders):\n",
    "#     train_decode = np.dot(spike_rates, decoders)\n",
    "#     plt.figure()\n",
    "#     plt.plot(train_input_rates, target_function, label=\"target function\")\n",
    "#     plt.plot(train_input_rates, train_decode, label=\"decoded function\")\n",
    "#     plt.legend(loc=\"best\")\n",
    "\n",
    "#     z_idx = np.searchsorted(train_input_rates[:, 0], 0) # input 0\n",
    "#     rates_0 = spike_rates[z_idx] # spike rates at input 0\n",
    "#     plt.figure()\n",
    "#     plt.hist(decoders[rates_0>0], density=True, bins=40)\n",
    "\n",
    "# plot_training_fit(tinfo.train_input_rates, tinfo.target_function, tinfo.spike_rates, tinfo.decoders.flatten())    "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
