{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Coverage for D > 1 on BD\n",
    "\n",
    "Meant to be the counterpart to the encoder coverage theory plot in the paper. \n",
    "A lot trickier to do with BD because estimating the encoders takes a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pystorm.hal import HAL\n",
    "from pystorm.PyDriver import bddriver as bd\n",
    "from pystorm.hal.net_builder import NetBuilder\n",
    "from pystorm.hal.calibrator import Calibrator, PoolSpec\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_random_unit_vector(D):\n",
    "    gaussian = np.random.randn(D)\n",
    "    return gaussian / np.linalg.norm(gaussian)\n",
    "\n",
    "def get_random_unit_vectors(D, N):\n",
    "    vects = np.zeros((N, D)) # same shape as encoders\n",
    "    for n in range(N):\n",
    "        vects[n,:] = get_random_unit_vector(D)\n",
    "    return vects\n",
    "\n",
    "def dot_to_angle(dots):\n",
    "    # 2 * a * b * cos(theta) = dot\n",
    "    return np.arccos(dots) / np.pi * 180\n",
    "        \n",
    "def get_max_dots_between(vects1, vects2):\n",
    "    # expecting NxD inputs\n",
    "    assert(vects1.shape[1] == vects2.shape[1])\n",
    "    \n",
    "    # do dot produce, get N1xN2 angle matrix\n",
    "    sim = np.dot(vects1, vects2.T)\n",
    "    \n",
    "    maxes1 = np.max(sim, axis=1)\n",
    "    maxes2 = np.max(sim, axis=0)\n",
    "    \n",
    "    return maxes1, maxes2\n",
    "\n",
    "def test_encoders(normed_encs, num_cmp_points):\n",
    "    # generate test points, get dot products\n",
    "    D = normed_encs.shape[1]\n",
    "    test_vs = get_random_unit_vectors(D, num_cmp_points)\n",
    "    enc_dots, test_v_dots = get_max_dots_between(normed_encs, test_vs)\n",
    "    return test_v_dots\n",
    "\n",
    "def get_performance(test_v_dots, pctile=.1):\n",
    "    sorted_dots = np.sort(test_v_dots)\n",
    "    p = sorted_dots[int(pctile * len(test_v_dots))]\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hal = HAL()\n",
    "cal = Calibrator(hal)\n",
    "\n",
    "D = 3\n",
    "Y, X = 16, 16\n",
    "N = Y * X\n",
    "LY, LX = 0, 0\n",
    "\n",
    "sanity_TPM = np.zeros((N, D))\n",
    "for y in range(0, Y, 2):\n",
    "    for x in range(0, X, 2):\n",
    "        if y > Y // 2:\n",
    "            n = y * X + x\n",
    "            sanity_TPM[n, 1] = 1\n",
    "sanity_TPM[0, 0] = 1\n",
    "sanity_TPM[2, 0] = 1\n",
    "sanity_TPM[4, 0] = 1\n",
    "sanity_TPM[6, 0] = 1\n",
    "\n",
    "#sanity_TPM[0 + 32, 2] = 1\n",
    "#sanity_TPM[2 + 32, 2] = 1\n",
    "#sanity_TPM[4 + 32, 2] = 1\n",
    "#sanity_TPM[6 + 32, 2] = 1\n",
    "\n",
    "## this one had something weird\n",
    "#sanity_TPM = np.zeros((N, D))\n",
    "#for y in range(0, Y, 2):\n",
    "#    for x in range(0, X, 2):\n",
    "#        n = y * X + x\n",
    "#        if (y // 2) % 2 == 0: # d+\n",
    "#            if x > X // 2: # d0\n",
    "#                sanity_TPM[n, 0] = 1\n",
    "#            else: # d1\n",
    "#                sanity_TPM[n, 1] = 1\n",
    "#        else: # d-\n",
    "#            if x > X // 2: # d0\n",
    "#                sanity_TPM[n, 0] = -1\n",
    "#            else: # d1\n",
    "#                sanity_TPM[n, 1] = -1\n",
    "\n",
    "#sanity_TPM = np.zeros((N, D))\n",
    "#for y in range(0, Y, 2):\n",
    "#    for x in range(0, X, 2):\n",
    "#        n = y * X + x\n",
    "#        if (y // 2) % 3 == 0: # d0\n",
    "#            sanity_TPM[n, 0] = 1\n",
    "#        if (y // 2) % 3 == 1: # d0\n",
    "#            sanity_TPM[n, 1] = 1\n",
    "#        if (y // 2) % 3 == 2: # d0\n",
    "#            sanity_TPM[n, 2] = 1\n",
    "#\n",
    "\n",
    "sanity_TPM_yx = sanity_TPM.reshape((Y, X, D))\n",
    "for d in range(D):\n",
    "    plt.figure()\n",
    "    plt.imshow(sanity_TPM_yx[:, :, d])\n",
    "\n",
    "#base_ps = PoolSpec(YX=(Y,X), loc_yx=(LY, LX), D=D, TPM=sanity_TPM)\n",
    "\n",
    "\n",
    "# ignore bad taps--yield doesn't matter here\n",
    "SY, SX = NetBuilder.to_synspace(Y, X)                                                   \n",
    "syn_yx_tap_matrix = NetBuilder.create_default_yx_taps(SY, SX, D, bad_syn=None)\n",
    "                                                                                        \n",
    "# reshape to NxD shape and make even                                                    \n",
    "default_TPM = NetBuilder.syn_taps_to_nrn_taps(syn_yx_tap_matrix)  \n",
    "NetBuilder.make_taps_even(default_TPM)\n",
    "\n",
    "base_ps = PoolSpec(YX=(Y,X), loc_yx=(LY, LX), D=D, TPM=default_TPM)\n",
    "\n",
    "#base_ps = PoolSpec(YX=(Y,X), loc_yx=(LY, LX), D=D)\n",
    "\n",
    "bad_syns, _ = cal.get_bad_syns()\n",
    "print(bad_syns.shape)\n",
    "pool_bad_syns = Calibrator.crop_calibration_to_pool(bad_syns, base_ps)  \n",
    "print(pool_bad_syns.shape)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(~pool_bad_syns)\n",
    "plt.title('good synapses')\n",
    "\n",
    "if D == 3:\n",
    "    DAC_vals = {\n",
    "            'DAC_SOMA_REF': 1024,\n",
    "            'DAC_DIFF_G': 1024,\n",
    "            'DAC_DIFF_R': 100}\n",
    "else:\n",
    "    DAC_vals = {}\n",
    "\n",
    "# yield doesn't actually come into play for the metric, \n",
    "# but we're going to be hard on ourselves and only include neurons that fire\n",
    "# not just 'good', for starters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_net(base_ps):\n",
    "    return cal.optimize_yield(base_ps, dacs=DAC_vals, \n",
    "                           bias_twiddle_policy='center', offset_source='calibration_db', validate=False, \n",
    "                           get_encs_kwargs={'solver':'scipy_opt'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#base_ps.TPM = opt_ps.TPM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "NUM_TRIALS = 1\n",
    "pss = []\n",
    "encs = []\n",
    "offs = []\n",
    "std_encs = []\n",
    "std_offs = []\n",
    "dbgs = []\n",
    "for i in range(NUM_TRIALS):\n",
    "    print(\"RUNNING TRIAL\", i)\n",
    "    print(\"==================\")\n",
    "    \n",
    "    #if i > 0:\n",
    "    #    base_ps.TPM = pss[0].TPM\n",
    "    \n",
    "    opt_ps, dac_vals, opt_encs, opt_offsets, std_opt_encs, std_opt_offsets, dbg = optimize_net(base_ps)\n",
    "    encs.append(opt_encs)\n",
    "    offs.append(opt_offsets)\n",
    "    std_encs.append(std_opt_encs)\n",
    "    std_offs.append(std_opt_offsets)\n",
    "    pss.append(opt_ps)\n",
    "    dbgs.append(dbg)\n",
    "    \n",
    "    # save progress\n",
    "    import pickle\n",
    "    pickle.dump((pss, encs, offs, std_encs, std_offs), open('save_encs_and_offs3.pck', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_net(opt_ps, opt_encs, opt_offsets, std_encs, std_offsets, scatter=False):\n",
    "    intercepts, good = cal.get_intercepts(opt_encs, opt_offsets)\n",
    "    plt.figure()\n",
    "    gains = cal.get_gains(opt_encs)\n",
    "    print(len(gains))\n",
    "    big_gain_mask = gains > 50\n",
    "    print(len(big_gain_mask))\n",
    "    big_gains = gains[big_gain_mask]\n",
    "    print(len(big_gains), 'substantial gains')\n",
    "\n",
    "    plt.figure()\n",
    "    plt.hist(big_gains, bins=40)\n",
    "\n",
    "    big_encs = opt_encs[big_gain_mask, :]\n",
    "    normed_big_encs = (big_encs.T / big_gains).T\n",
    "    \n",
    "    big_std_encs = std_encs[big_gain_mask, :]\n",
    "    normed_std_encs = (big_std_encs.T / big_gains).T\n",
    "\n",
    "    if scatter:\n",
    "        \n",
    "        def make_one_scatter(ax, encs, d0, d1, std_encs=None):\n",
    "            m = np.max(np.abs(encs[~np.isnan(encs)]))\n",
    "            if m > 500:\n",
    "                m = 500 \n",
    "            ax.axis([-m, m, -m, m])\n",
    "            ax.scatter(encs[:, d0], encs[:, d1], s=4)\n",
    "            if std_encs is not None:\n",
    "                from matplotlib.patches import Ellipse\n",
    "                for i in range(encs.shape[0]):\n",
    "                    r0 = std_encs[i, d0] * 2 # 2-sigma ~ 95%\n",
    "                    r1 = std_encs[i, d1] * 2\n",
    "                    ell = Ellipse((encs[i, d0], encs[i, d1]), r0, r1)\n",
    "                    ax.add_artist(ell)\n",
    "                    ell.set_alpha(.2)\n",
    "                    ell.set_facecolor('orange')\n",
    "            \n",
    "        if D == 2:\n",
    "            fig, ax = plt.subplots(1, 2, figsize=(2*7, 7))\n",
    "            make_one_scatter(ax[0], big_encs, 0, 1, std_encs=std_encs)\n",
    "            make_one_scatter(ax[1], normed_big_encs, 0, 1, std_encs=normed_std_encs)\n",
    "        if D == 3:\n",
    "            fig, ax = plt.subplots(3, 2, figsize=(2*7, 3*7))\n",
    "            make_one_scatter(ax[0, 0], big_encs, 0, 1, std_encs)\n",
    "            make_one_scatter(ax[1, 0], big_encs, 1, 2, std_encs)\n",
    "            make_one_scatter(ax[2, 0], big_encs, 2, 0, std_encs)\n",
    "            make_one_scatter(ax[0, 1], normed_big_encs, 0, 1, normed_std_encs)\n",
    "            make_one_scatter(ax[1, 1], normed_big_encs, 1, 2, normed_std_encs)\n",
    "            make_one_scatter(ax[2, 1], normed_big_encs, 2, 0, normed_std_encs)\n",
    "    \n",
    "    plt.figure(figsize=(5,5))\n",
    "    all_tap_ax = plt.gca()\n",
    "    \n",
    "    fig, ax = plt.subplots(1, D+1, figsize=(3*D, 3), gridspec_kw={'width_ratios':[20]*D + [1]})\n",
    "    for d in range(D):\n",
    "        thr_encs = opt_encs[:, d].copy()\n",
    "        thr_encs[np.isnan(thr_encs)] = 0\n",
    "        #im = ax[d].imshow(thr_encs.reshape(Y, X), vmin=-800, vmax=800)\n",
    "        \n",
    "        log_encs = thr_encs.copy()\n",
    "        log_encs[log_encs > 0] = np.log(log_encs[log_encs > 0] + 1)\n",
    "        log_encs[log_encs < 0] = -np.log(-log_encs[log_encs < 0] + 1)\n",
    "        \n",
    "        im = ax[d].imshow(log_encs.reshape(Y, X), vmin=-np.log(800), vmax=np.log(800))\n",
    "        \n",
    "        if d == D-1:\n",
    "            plt.colorbar(im, cax=ax[d+1])\n",
    "        \n",
    "        pos_taps = []\n",
    "        neg_taps = []\n",
    "        for x in range(opt_ps.X):\n",
    "            for y in range(opt_ps.Y):\n",
    "                n = y * X + x\n",
    "                xsyn = x\n",
    "                if (y // 2) % 2 == 1:\n",
    "                    xsyn += 1\n",
    "                if opt_ps.TPM[n, d] == 1:\n",
    "                    pos_taps.append([y, xsyn])\n",
    "                if opt_ps.TPM[n, d] == -1:\n",
    "                    neg_taps.append([y, xsyn])\n",
    "        pos_taps = np.array(pos_taps)\n",
    "        neg_taps = np.array(neg_taps)\n",
    "        if len(pos_taps) > 0:\n",
    "            ax[d].scatter(pos_taps[:, 1], pos_taps[:, 0], marker='+', c='r')\n",
    "        if len(neg_taps) > 0:\n",
    "            ax[d].scatter(neg_taps[:, 1], neg_taps[:, 0], marker='+', c='cyan')\n",
    "            \n",
    "        dim_colors = ['r', 'g', 'b', 'cyan', 'magenta', 'yellow']\n",
    "        all_tap_ax.scatter(pos_taps[:, 1], pos_taps[:, 0], marker='x', c=dim_colors[d])\n",
    "        all_tap_ax.scatter(neg_taps[:, 1], neg_taps[:, 0], marker='o', c=dim_colors[d])\n",
    "    plt.tight_layout(w_pad=.05, h_pad=.05)\n",
    "            \n",
    "        \n",
    "    num_pts = 1000\n",
    "    dots = test_encoders(normed_big_encs, num_pts)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(np.sort(dots))\n",
    "    plt.axvline(.1 * num_pts)\n",
    "\n",
    "    return get_performance(dots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pss, encs, offs, std_encs, std_offs = pickle.load(open('save_encs_and_offs3.pck', 'rb'))\n",
    "perfs = []\n",
    "for i, ps, enc, off, std_enc, std_off in zip(range(len(encs)), pss, encs, offs, std_encs, std_offs):\n",
    "    print(\"ANALYZING TRIAL\", i)\n",
    "    print(\"==================\")\n",
    "    perfs.append(analyze_net(ps, enc, off, std_enc, std_off, scatter=True))\n",
    "print(perfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spikes = dbgs[0]['spikes']\n",
    "sample_pts = dbgs[0]['sample_pts']\n",
    "print(spikes.shape)\n",
    "print(sample_pts.shape)\n",
    "plt.figure()\n",
    "gains = Calibrator.get_gains(encs[0])\n",
    "nrn = 32\n",
    "print('gain', gains[nrn])\n",
    "plt.tricontourf(sample_pts[:, 0], sample_pts[:, 1], spikes[:, nrn])\n",
    "plt.colorbar()\n",
    "print(encs[0][nrn, :])\n",
    "plt.figure()\n",
    "plt.plot(spikes[:, nrn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(perfs, bins=20)\n",
    "print(max(perfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# neurons get more juice regardless of whether dim 0 or dim 1 is driven\n",
    "# both SGs target the same place in TAT?\n",
    "# SG0 shouldn't target anything?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sum(np.abs(pss[0].TPM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ints, gains = Calibrator.get_intercepts(encs[0], offs[0])\n",
    "plt.figure()\n",
    "plt.hist(ints[~np.isnan(ints)], range=(-4, 0), bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- improve algorithm to be smarter when taps are empty:\n",
    "    - or just don't ignore 'bad taps' -- easy, maybe do this for now\n",
    "    - should actually spend some time with the basic algorithm. Doesn't seem to do a good job keeping taps/dim \n",
    "    relatively constant\n",
    "    - should perhaps bootstrap the 'best' taps results from the simulation to the chip\n",
    "        - need to resurrect notebook. would be good to do that before Ben gets his hands on it\n",
    "- need to get a handle on the encoder estimation error thing, wrt presenting results in paper\n",
    "    - bootstrapping to get the best performance's variance probably isn't actually the right thing to do\n",
    "    - bootstrapping the error of a GIVEN encoder might be a better thing to do\n",
    "    - since the statistic is on the closest encoder, this tells that the closet encoder could be up to this much \n",
    "    - farther away\n",
    "    \n",
    "### Game Plan \n",
    "    \n",
    "1. finish bootstrap\n",
    "2. see if 3^D -> 2^D points tanks estimation quality\n",
    "3. update CDF to use 2-sigma worst-case\n",
    "    - figure out how many samples are needed for reasonably distance from nominal performance\n",
    "4. go to unconstrained anchor encoders\n",
    "    - add in pre-optimization simulation\n",
    "5. off to the races!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
