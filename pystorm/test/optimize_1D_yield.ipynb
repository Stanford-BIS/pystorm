{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from pystorm.hal import HAL\n",
    "from pystorm.PyDriver import bddriver as bd\n",
    "from pystorm.hal.net_builder import NetBuilder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# making a Y-by-X pool of D dims located at (LY, LX)\n",
    "\n",
    "Y, X = (16, 16)\n",
    "LY, LX = (16, 0)\n",
    "\n",
    "N = X * Y\n",
    "D = 1 # has to be 1 for this\n",
    "\n",
    "SY = Y // 2\n",
    "SX = X // 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hal = HAL()\n",
    "net_builder = NetBuilder(hal)\n",
    "\n",
    "# get the complete set of bad syn locations for the chip (slow PEs and high bias mismatch)\n",
    "bad_syns, _ = net_builder.determine_bad_syns()\n",
    "\n",
    "# create some nice taps, avoiding bad syns\n",
    "syn_tap_matrix = NetBuilder.create_default_yx_taps(Y // 2, X // 2, D, \n",
    "                                                  bad_syn=bad_syns[LY // 2 : (LY + Y) // 2, \n",
    "                                                                   LX // 2 : (LX + X) // 2])\n",
    "nrn_tap_matrix = NetBuilder.syn_taps_to_nrn_taps(syn_tap_matrix)\n",
    "NetBuilder.make_taps_even(nrn_tap_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine if syn_tap_matrix is right/left or up/down +1/-1 \n",
    "# so I know how to cut it in two\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(syn_tap_matrix.reshape(SY, SX))\n",
    "plt.title('corner coordinates\\nCCW from top-left: (0, 0), (SY, 0), (SY, SX), (0, SX)')\n",
    "\n",
    "print(syn_tap_matrix[0, 0])\n",
    "print(syn_tap_matrix[SY-1, 0])\n",
    "print(syn_tap_matrix[0, SX-1])\n",
    "print(syn_tap_matrix[SY-1, SX-1])\n",
    "\n",
    "# need diffusor cuts in terms of tile_ids\n",
    "# cuts are on the north and west sides, left/right and top/bottom respectively\n",
    "# we're making a vertical cut, so using west top/bottoms\n",
    "# 4 syns to a tile\n",
    "\n",
    "DIFFUSOR_NORTH_LEFT = bd.bdpars.DiffusorCutLocationId.NORTH_LEFT\n",
    "DIFFUSOR_NORTH_RIGHT = bd.bdpars.DiffusorCutLocationId.NORTH_RIGHT\n",
    "DIFFUSOR_WEST_TOP = bd.bdpars.DiffusorCutLocationId.WEST_TOP\n",
    "DIFFUSOR_WEST_BOTTOM = bd.bdpars.DiffusorCutLocationId.WEST_BOTTOM\n",
    "\n",
    "def slice_square_pool_in_half(hal):\n",
    "    x_off = LX // 2 // 2\n",
    "    y_off = LY // 2 // 2\n",
    "    num_tiles_vert = SY // 2\n",
    "    x_idx = SX // 2 // 2 #east of midline tile idx\n",
    "    \n",
    "    \n",
    "    for y_idx in range(num_tiles_vert):\n",
    "        #print(x_idx + x_off, y_idx + y_off)\n",
    "        hal.driver.OpenDiffusorCutXY(0, x_idx + x_off, y_idx + y_off, DIFFUSOR_WEST_TOP)\n",
    "        hal.driver.OpenDiffusorCutXY(0, x_idx + x_off, y_idx + y_off, DIFFUSOR_WEST_BOTTOM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up the Network object, do encoder/offset optimization\n",
    "def run_network(diff_G, diff_R=1024, cut='none', biases=0, tap_matrix=nrn_tap_matrix):\n",
    "    net = net_builder.create_single_pool_net(Y, X, loc_yx=(LY, LX), tap_matrix=tap_matrix, biases=biases, gain_divs=1)\n",
    "    pool_obj = net.get_pools()[0]\n",
    "    input_obj = net.get_inputs()[0]\n",
    "    hal.map(net)\n",
    "    \n",
    "    hal.set_DAC_value('DAC_SOMA_REF', 1024)\n",
    "\n",
    "    # make the diffusor broad\n",
    "    hal.set_DAC_value('DAC_DIFF_G', diff_G)\n",
    "    hal.set_DAC_value('DAC_DIFF_R', diff_R)\n",
    "\n",
    "    # net is now mapped, try slicing the diffusor\n",
    "    if cut == 'line':\n",
    "        slice_square_pool_in_half(hal)\n",
    "    elif cut == 'all':\n",
    "        net_builder.open_all_diff_cuts()\n",
    "\n",
    "    # determine fmax\n",
    "    safe_fmaxes = net_builder.determine_safe_fmaxes()\n",
    "    fmax = safe_fmaxes[pool_obj]\n",
    "\n",
    "    # estimate encoders and offsets\n",
    "    encoders, offsets, _, _ = net_builder.determine_encoders_and_offsets(pool_obj, input_obj, fmax, \n",
    "                                                                         num_sample_angles=3,\n",
    "                                                                         solver='scipy_opt')\n",
    "    \n",
    "    return net, encoders, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# do exhaustive twiddle val search, like Terry does\n",
    "# TODO come back and try with the calibration\n",
    "\n",
    "import pickle\n",
    "pck_fname = 'raw_offsets.pck'\n",
    "REDO_SWEEP = False\n",
    "\n",
    "if REDO_SWEEP:\n",
    "    raw_offsets = np.zeros((7, N))\n",
    "    for bias_idx, bias in enumerate([-3, -2, -1, 0, 1, 2, 3]):\n",
    "        trick_net, trick_encoders, trick_offsets = run_network(diff_G=1024, diff_R=1024, cut='line', biases=bias)\n",
    "        raw_offsets[bias_idx, :] = trick_offsets\n",
    "    pickle.dump(raw_offsets, open(pck_fname, 'wb'))\n",
    "else:\n",
    "    raw_offsets = pickle.load(open(pck_fname, 'rb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make it like the soma_bias_twiddle calibration, relative to bias twiddle 0\n",
    "all_offsets = raw_offsets.copy()\n",
    "orig_offsets_at_3 = raw_offsets[6, :]\n",
    "orig_offsets_at_0 = raw_offsets[3, :]\n",
    "    \n",
    "# subtract out 0 bias\n",
    "for bias_idx, bias in enumerate([-3, -2, -1, 0, 1, 2, 3]):\n",
    "    all_offsets[bias_idx, :] -= orig_offsets_at_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot resulting offsets\n",
    "plt.figure()\n",
    "plt.plot(raw_offsets)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_offsets)\n",
    "\n",
    "# estimate slope of each neurons sampled twiddle levels\n",
    "# the mismatch of each level is correlated\n",
    "# this is a better way of estimating unsampled bias levels than what \n",
    "# the calibration does\n",
    "\n",
    "def idx_to_mag(idx):\n",
    "    return np.abs(idx - 3)\n",
    "\n",
    "est_slope_p = np.zeros(N)\n",
    "highest_sample_p = np.zeros(N, dtype=int)\n",
    "est_slope_n = np.zeros(N)\n",
    "highest_sample_n = np.zeros(N, dtype=int)\n",
    "for n in range(N):\n",
    "    nrn_offsets = all_offsets[:, n]\n",
    "    valid = ~np.isnan(nrn_offsets)\n",
    "    valid_off = nrn_offsets.copy()\n",
    "    valid_off[~valid] = 0\n",
    "    if len(valid_off) > 0:\n",
    "        biggest = np.max(valid_off)\n",
    "        biggest_arg = np.argmax(valid_off)\n",
    "        if biggest > 0 and biggest_arg > 3:\n",
    "            est_slope_p[n] = biggest / idx_to_mag(biggest_arg)\n",
    "            highest_sample_p[n] = idx_to_mag(biggest_arg)\n",
    "            \n",
    "        smallest = np.min(valid_off)\n",
    "        smallest_arg = np.argmin(valid_off)\n",
    "        if smallest < 0 and smallest_arg < 3:\n",
    "            est_slope_n[n] = smallest / idx_to_mag(smallest_arg)\n",
    "            highest_sample_n[n] = idx_to_mag(smallest_arg)\n",
    "            \n",
    "# compute global mean offsets\n",
    "means = []\n",
    "for bias_level in range(7):\n",
    "    bias_offsets = all_offsets[bias_level, :]\n",
    "    means.append(np.mean(bias_offsets[~np.isnan(bias_offsets)]))\n",
    "    \n",
    "# weight estimated slopes and global means together to fill in data\n",
    "all_offsets_est = all_offsets.copy()\n",
    "for n in range(N):\n",
    "    for bias_level in range(7):\n",
    "        if np.isnan(all_offsets_est[bias_level, n]):\n",
    "            assign = False\n",
    "            if bias_level > 3:\n",
    "                highest_sample = highest_sample_p[n]\n",
    "                est_slope = est_slope_p[n]\n",
    "                if idx_to_mag(bias_level) > highest_sample:\n",
    "                    assign = True\n",
    "            elif bias_level < 3:\n",
    "                highest_sample = highest_sample_n[n]\n",
    "                est_slope = est_slope_n[n]\n",
    "                if idx_to_mag(bias_level) > highest_sample:\n",
    "                    assign = True\n",
    "            if assign:\n",
    "                nrn_est_offset = idx_to_mag(bias_level) * est_slope\n",
    "                all_offsets_est[bias_level, n] = nrn_est_offset * highest_sample / 3 + \\\n",
    "                                                 means[bias_level] * (1 - highest_sample / 3)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(all_offsets_est)\n",
    "print('hi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def optimize_twiddles_once(cut, diff_G, diff_R, tap_matrix=None):\n",
    "\n",
    "    def run_curr_network(biases):\n",
    "        if tap_matrix is None:\n",
    "            return run_network(diff_G, diff_R=diff_R, cut=cut, biases=biases)\n",
    "        else:\n",
    "            return run_network(diff_G, diff_R=diff_R, cut=cut, biases=biases, tap_matrix=tap_matrix)\n",
    "\n",
    "    _, encoders, offsets = run_curr_network(3)\n",
    "    offsets_at_3 = offsets\n",
    "\n",
    "    bias_settings, new_offsets, good, bin_counts, dbg = \\\n",
    "        NetBuilder.pick_good_twiddles(encoders, offsets_at_3, all_offsets_est, policy='greedy_flat')\n",
    "\n",
    "    fs = (15, 15)\n",
    "    xylim = (0, 800, -800, 1500)\n",
    "    NetBuilder.plot_neuron_yield_cone(encoders, new_offsets, good,\n",
    "                                     (encoders, offsets, bias_settings),\n",
    "                                      title='expected',\n",
    "                                      figsize=fs,\n",
    "                                      xylim=xylim)\n",
    "\n",
    "    net_opt, encoders_opt, offsets_opt = run_curr_network(bias_settings)\n",
    "\n",
    "    NetBuilder.plot_neuron_yield_cone(encoders_opt, offsets_opt, good,\n",
    "                                     (encoders, offsets, bias_settings),\n",
    "                                      title='trick opt vs trick',\n",
    "                                      figsize=fs,\n",
    "                                      xylim=xylim)\n",
    "\n",
    "    good_orig = np.sum(NetBuilder.get_good_mask(encoders, offsets))\n",
    "    good_exp = np.sum(NetBuilder.get_good_mask(encoders, new_offsets))\n",
    "    good_ver = np.sum(NetBuilder.get_good_mask(encoders_opt, offsets_opt))\n",
    "    print('good orig:', good_orig)\n",
    "    print('good exp:', good_exp)\n",
    "    print('good ver:', good_ver)\n",
    "    \n",
    "    return encoders_opt, offsets_opt, good_ver, net_opt\n",
    "        \n",
    "def do_validation_exp(encoders_opt, offsets_opt, net_opt):\n",
    "    NUM_VAL_SAMPLES = 20\n",
    "    val_pts = np.linspace(-1, 1, NUM_VAL_SAMPLES).reshape((NUM_VAL_SAMPLES, 1))\n",
    "\n",
    "    pool_obj = net_opt.get_pools()[0]\n",
    "    inp_obj = net_opt.get_inputs()[0]\n",
    "    safe_fmaxes = net_builder.determine_safe_fmaxes()\n",
    "    fmax = safe_fmaxes[pool_obj]\n",
    "    rmse, meas_A, est_A = net_builder.validate_est_encs(encoders_opt, offsets_opt, \n",
    "                                                        pool_obj, inp_obj, val_pts, fmax)\n",
    "    return val_pts, meas_A\n",
    "\n",
    "def plot_validation_exp(encoders_opt, offsets_opt, val_pts, meas_A, lines_per_plot=None):\n",
    "\n",
    "    clean_encs = encoders_opt.copy()\n",
    "    unest = np.isnan(offsets_opt)\n",
    "    clean_encs[unest, :] = 0\n",
    "    clean_offsets = offsets_opt.copy()\n",
    "    clean_offsets[unest] = 0\n",
    "\n",
    "    est_A = np.maximum(0, np.dot(val_pts, clean_encs.T) + clean_offsets)\n",
    "\n",
    "    if lines_per_plot is not None:\n",
    "        n_neurons = meas_A.shape[1]\n",
    "        plot_nrn_idxs = np.random.permutation(np.arange(n_neurons))[:lines_per_plot]\n",
    "        print(plot_nrn_idxs)\n",
    "        meas_A_plot = meas_A[:, plot_nrn_idxs]\n",
    "        est_A_plot = est_A[:, plot_nrn_idxs]\n",
    "    else:\n",
    "        meas_A_plot = meas_A\n",
    "        est_A_plot = est_A\n",
    "        \n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.gca().set_prop_cycle(None)\n",
    "    plt.plot(val_pts, meas_A_plot, '.-')\n",
    "    plt.gca().set_prop_cycle(None)\n",
    "    plt.plot(val_pts, est_A_plot, '-')\n",
    "    plt.axis([-1, 1, 0, 500]) \n",
    "    plt.title('estimated vs measured tuning curves')\n",
    "    \n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.plot(val_pts, est_A)\n",
    "    plt.axis([-1, 1, 0, 500]) \n",
    "    plt.title('estimated tuning curves')\n",
    "\n",
    "    plt.figure()\n",
    "    is_est = ~unest\n",
    "    gains = NetBuilder.get_gains(encoders_opt[is_est, :])\n",
    "    intercepts = -offsets_opt[is_est] / gains\n",
    "    plt.hist(intercepts, bins=np.linspace(-2, 2, 21))\n",
    "    plt.title('intercept distribution')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "encs, offs, good_ct, net = optimize_twiddles_once('line', 1024, 1024)\n",
    "val_pts, meas_A = do_validation_exp(encs, offs, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_validation_exp(encs, offs, val_pts, meas_A, lines_per_plot=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sweep diffusor spread, measuring for yield\n",
    "def run_diffusor_sweep(cut, diff_Gs, diff_Rs, tap_matrix=None):\n",
    "    encs = []\n",
    "    offs = []\n",
    "    good_cts = []\n",
    "    for diff_G, diff_R in zip(diff_Gs, diff_Rs):\n",
    "        optimize_twiddles_once(cut, diff_G, diff_R, tap_matrix)\n",
    "        \n",
    "        encs.append(encoders_opt)\n",
    "        offs.append(encoders_opt)\n",
    "        good_cts.append(good_ver)\n",
    "        \n",
    "    return encs, offs, good_cts, net_opt\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diff_Rs = [500, 1024, 1024]\n",
    "diff_Gs = [1024, 1024, 500]\n",
    "cut = 'line'\n",
    "run_diffusor_sweep(cut, diff_Gs, diff_Rs, plot_est_encs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diff_Rs = [1024, 500, 100]\n",
    "diff_Gs = [1024]*len(diff_Rs)\n",
    "cut = 'none'\n",
    "run_diffusor_sweep(cut, diff_Gs, diff_Rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# see how much bad_syns helps\n",
    "\n",
    "default_syn_tap_matrix = NetBuilder.create_default_yx_taps(Y // 2, X // 2, D)\n",
    "default_nrn_tap_matrix = NetBuilder.syn_taps_to_nrn_taps(default_syn_tap_matrix)\n",
    "NetBuilder.make_taps_even(default_nrn_tap_matrix)\n",
    "\n",
    "diff_Rs = [1024]\n",
    "diff_Gs = [1024]\n",
    "\n",
    "cut = 'line'\n",
    "run_diffusor_sweep(cut, diff_Gs, diff_Rs, tap_matrix=default_nrn_tap_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
